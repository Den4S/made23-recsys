{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrjZ-Ma78a0c"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CmEukeg8Njd"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import tqdm\n",
    "import json\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "import tensorboardX as tb\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(31337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4cLf0zW8Njf"
   },
   "source": [
    "## Create pairs (first track, subsequent track, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DHUIFjU0Z09C"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKlgAqq-8Njg"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/content/drive/MyDrive/recsys-itmo-2023/seminar_05/\"\n",
    "# данные, собранные с помощью lightFM рекомендера (10,000 сессий)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aeehkP8Njh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(DATA_DIR + \"data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj9JftT88Njh"
   },
   "outputs": [],
   "source": [
    "Pair = namedtuple(\"Session\", [\"user\", \"start\", \"track\", \"time\"])\n",
    "\n",
    "# разбиваем сессии на пары\n",
    "def get_pairs(user_data):\n",
    "    pairs = []\n",
    "    first = None\n",
    "    for _, row in user_data.sort_values(\"timestamp\").iterrows():\n",
    "        if first is None:\n",
    "            first = row[\"track\"]\n",
    "        else:\n",
    "            pairs.append(Pair(row[\"user\"], first, row[\"track\"], row[\"time\"]))\n",
    "        \n",
    "        if row[\"message\"] == \"last\":\n",
    "            first = None\n",
    "    return pairs  # получаем для каждого пользователя пары прослушанных треков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем учиться предсказывать время прослушивания второго трека в паре\n",
    "# пользователь сам выбирает первый трек!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c_Ifi9_8Nji"
   },
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(\n",
    "    data\n",
    "    .groupby(\"user\")\n",
    "    .apply(get_pairs)\n",
    "    .explode()\n",
    "    .values\n",
    "    .tolist(),\n",
    "    columns=[\"user\", \"start\", \"track\", \"time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA0LzG3Z8Nji"
   },
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots()\n",
    "sns.histplot(pairs[\"time\"], ax=ax)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkYDflFK8Njj"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE63YQAi8Njj"
   },
   "outputs": [],
   "source": [
    "rdm = np.random.random(len(pairs))\n",
    "train_data = pairs[rdm < 0.8]\n",
    "val_data = pairs[(rdm >= 0.8) & (rdm < 0.9)]\n",
    "test_data = pairs[rdm >= 0.9]  # train - 0.9, test - 0.1\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N72w3Ym8Njl"
   },
   "outputs": [],
   "source": [
    "class ContextualRanker(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim=10):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim  # размерность эмбеддинга\n",
    "        \n",
    "        # We won't have embeddings for everything, but that's ok\n",
    "        # хранилища с эмбеддингами (всего N=50000 треков)\n",
    "        self.context = nn.Embedding(num_embeddings=50000, embedding_dim=self.embedding_dim)\n",
    "        self.track = nn.Embedding(num_embeddings=50000, embedding_dim=self.embedding_dim)\n",
    "        # кол-во параметров: 1M\n",
    "        # два разных эмбеддинга очим т.к. треки могут быть как стартовыми, так и первыми в сессии\n",
    "        # 1. трек является стартовым\n",
    "        # 2. трек не является стартовым\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - это пара\n",
    "        context = self.context(x[:, 0])  # start track\n",
    "        track = self.track(x[:, 1])  # next track\n",
    "        # просто перемножаем два эмбеддинга?!\n",
    "        return torch.sum(context * track, dim=1)  # скалярное произведение\n",
    "            \n",
    "    def step(self, batch, batch_idx, metric, prog_bar=False):\n",
    "        # для каждого батча получаем предсказания\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        # MSE подходит для регрессии\n",
    "        loss = F.mse_loss(predictions, y.float(), reduction='mean')\n",
    "        self.log(metric, loss, prog_bar=prog_bar)  # логирование лосса\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=False):\n",
    "        # во время тестирования считаем три разных лосса\n",
    "        x, y = batch\n",
    "        predictions = self.forward(x)\n",
    "        targets = y[:, 0].float()  # таргеты\n",
    "        avgs = y[:, 1].float()  # среднее\n",
    "        rdms = y[:, 2].float()  # случайное\n",
    "\n",
    "        loss = F.mse_loss(predictions, targets, reduction='mean')\n",
    "        avg_loss = F.mse_loss(avgs, targets, reduction='mean')\n",
    "        rdm_loss = F.mse_loss(rdms, targets, reduction='mean')\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=prog_bar)  # логируем\n",
    "        self.log(\"avg_loss\", avg_loss, prog_bar=prog_bar)\n",
    "        self.log(\"rdm_loss\", rdm_loss, prog_bar=prog_bar)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # шаг обучения\n",
    "        return self.step(batch, batch_idx, \"train_loss\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # шаг валидации\n",
    "        return self.step(batch, batch_idx, \"val_loss\", True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # используем адам, т.к. хорошо ведет себя с тяжелыми хвостами?\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        # уменьшаем LR если в течение трех последних эпох не происходило изменения лосса\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "        scheduler = {\n",
    "            'scheduler': lr_scheduler,\n",
    "            'reduce_on_plateau': True,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSZTEW7h9d3p"
   },
   "outputs": [],
   "source": [
    "class ContextualRankerData(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, features):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.features = features\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        метод для скачивания и предобработки\n",
    "            докидываем две доп. колонки для дальнейшего подсчета лосса и сравнения\n",
    "        \"\"\"\n",
    "        self.test_data = self.test_data.assign(rdm = np.random.random(len(self.test_data))).assign(avg = self.train_data[\"time\"].mean())\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "        self.train_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.train_data[self.features].values),  # фичи\n",
    "            torch.from_numpy(self.train_data[\"time\"].values)  # таргеты\n",
    "            )\n",
    "        \n",
    "        self.val_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.val_data[self.features].values), \n",
    "            torch.from_numpy(self.val_data[\"time\"].values)\n",
    "            )\n",
    "\n",
    "        if stage == \"test\" or stage is None:  \n",
    "        self.test_dataset = td.TensorDataset(\n",
    "            torch.from_numpy(self.test_data[self.features].values),\n",
    "            torch.from_numpy(self.test_data[[\"time\", \"avg\", \"rdm\"]].values)  # закидываем случайное и среднее времена\n",
    "        )\n",
    "    def train_dataloader(self):\n",
    "        # данные при делении на батчи лучше перемешивать\n",
    "        return td.DataLoader(self.train_dataset, batch_size=2048, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return td.DataLoader(self.val_dataset, batch_size=2048, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return td.DataLoader(self.test_dataset, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWZ8cqTZ8Njm"
   },
   "outputs": [],
   "source": [
    "net = ContextualRanker(embedding_dim=100)\n",
    "data_module = ContextualRankerData(train_data, val_data, test_data, features = [\"start\", \"track\"])\n",
    "\n",
    "# checkpoint: сохраняем модель и выбираем лучшую по минимальному лоссу\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=300,\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        # останавливаем обучение, если 5 эпох ничего не меняется\n",
    "        pl.callbacks.early_stopping.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "        # логируем LR\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "        checkpoint_callback\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omCmoxVhGfJ2"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/lightning_logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqy8qDr98Njm"
   },
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    net,  # инстанс сети\n",
    "    data_module  # инстанс данных\n",
    ")  # обучаемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IeB7jzb8Njn"
   },
   "outputs": [],
   "source": [
    "# сохраняем модельку\n",
    "best = ContextualRanker.load_from_checkpoint(checkpoint_callback.best_model_path, embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTUgc8_hQ7N0"
   },
   "outputs": [],
   "source": [
    "# тестируем: наша модель справляется лучше, чем рандомное и среднее предсказание\n",
    "trainer.test(best, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRZLUR9_8Njo"
   },
   "source": [
    "## Compute top recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UrLSjgt8Njo"
   },
   "outputs": [],
   "source": [
    "track_meta = pd.read_json(DATA_DIR + \"tracks.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqwVsVyO8Njp"
   },
   "outputs": [],
   "source": [
    "# для каждого трека по id хотим подобрать какое-то кол-во id ближайших соседей\n",
    "context_embeddings = dict(best.named_parameters())[\"context.weight\"].data.cpu().numpy()\n",
    "track_embeddings = dict(best.named_parameters())[\"track.weight\"].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z1VTPGh8Njp"
   },
   "outputs": [],
   "source": [
    "track_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7_A20no8Njp"
   },
   "outputs": [],
   "source": [
    "k = 100\n",
    "with open(DATA_DIR + \"tracks_with_recs.json\", \"w\") as rf:\n",
    "    for _, track in tqdm.tqdm(track_meta.iterrows()):\n",
    "        embedding = context_embeddings[track[\"track\"]]  # эмбеддинг, когда является стартовым\n",
    "        # ищем соседей по эмбеддингу из другого хранилища эмбеддингов track_embeddings\n",
    "        neighbours = np.argpartition(-np.dot(track_embeddings, embedding), k)[:k]\n",
    "        \n",
    "        recommendation = dict(track)\n",
    "        recommendation[\"recommendations\"] = neighbours.tolist()\n",
    "        \n",
    "        rf.write(json.dumps(recommendation) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhS_o_ff8Njp"
   },
   "outputs": [],
   "source": [
    "track = 3916\n",
    "embedding = context_embeddings[track]\n",
    "track_meta.loc[track_meta[\"track\"] == track, [\"artist\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGFHEN8g8Njq"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "neighbours = np.argpartition(-np.dot(track_embeddings, embedding), k)[:k]\n",
    "track_meta.loc[track_meta[\"track\"].isin(neighbours), [\"artist\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymCblQht8Njq"
   },
   "outputs": [],
   "source": [
    "# сохраняем подсчитанные рекомендации"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
